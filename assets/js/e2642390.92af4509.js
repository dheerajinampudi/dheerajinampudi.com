"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[558],{5410:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>g,frontMatter:()=>o,metadata:()=>t,toc:()=>c});var t=i(9222),s=i(4848),r=i(8453);const o={slug:"ai-tuning",title:"Stages of AI Tuning",date:new Date("2024-12-07T00:00:00.000Z")},a="Stages of AI Tuning",l={authorsImageUrls:[]},c=[{value:"Overview of the Stages",id:"overview-of-the-stages",level:2},{value:"Visualizing the Spectrum",id:"visualizing-the-spectrum",level:2},{value:"Detailed Comparison",id:"detailed-comparison",level:2},{value:"Choosing the Right Approach",id:"choosing-the-right-approach",level:2}];function d(e){const n={br:"br",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"The world of AI customization involves a spectrum of approaches ranging from zero-cost prompt engineering to fully-fledged pre-training of large language models (LLMs) from scratch. Each stage offers varying levels of complexity, data requirements, and organizational investment. Understanding these stages can help you choose the right approach for your specific business or research needs."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-the-stages",children:"Overview of the Stages"}),"\n",(0,s.jsx)(n.p,{children:"The main strategies for tuning or customizing an AI model include:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"}),(0,s.jsx)(n.br,{}),"\n","At the simplest end of the spectrum lies ",(0,s.jsx)(n.strong,{children:"prompt engineering"}),". Instead of altering the model\u2019s parameters, you craft carefully worded prompts to guide the model\u2019s outputs. This approach requires no additional data or lengthy training sessions. It\u2019s quick, cost-effective, and provides immediate improvements, but offers less fine-grained control."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Retrieval Augmented Generation (RAG)"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"RAG"})," introduces external knowledge sources\u2014such as a vector database or an up-to-date knowledge base\u2014into the model\u2019s responses. By retrieving relevant information at query time, the model can deliver more accurate and contextually rich answers without retraining. While it increases complexity and inference costs, RAG remains a powerful method to dynamically update the model\u2019s \u201cknowledge\u201d on the fly."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fine-Tuning"}),(0,s.jsx)(n.br,{}),"\n","For organizations seeking more specialized control, ",(0,s.jsx)(n.strong,{children:"fine-tuning"})," is the next level. By training a pre-existing LLM on your domain-specific data, you can refine its behavior and outputs. Although it requires labeled data, computational resources, and a more extended training period than prompt engineering or RAG, fine-tuning grants granular control over the model\u2019s performance and can significantly improve domain-specific accuracy."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pre-Training"}),(0,s.jsx)(n.br,{}),"\n","At the most resource-intensive end of the spectrum is ",(0,s.jsx)(n.strong,{children:"pre-training"})," an LLM from scratch on massive datasets. This approach provides maximum control and a tailor-made model architecture perfectly aligned with your unique needs. However, it demands significant time (often weeks or months), computational power, and large-scale curated data. It\u2019s typically pursued by large organizations or research teams aiming for pioneering innovations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visualizing-the-spectrum",children:"Visualizing the Spectrum"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Stages of AI Tuning",src:i(7580).A+"",width:"1024",height:"768"})}),"\n",(0,s.jsxs)(n.p,{children:["for a visual illustration of these stages plotted against factors like ",(0,s.jsx)(n.strong,{children:"Time/Complexity"})," and ",(0,s.jsx)(n.strong,{children:"Response Quality/Organizational Maturity"}),". As you move from ",(0,s.jsx)(n.strong,{children:"Prompt Engineering"})," to ",(0,s.jsx)(n.strong,{children:"RAG"}),", and then on to ",(0,s.jsx)(n.strong,{children:"Fine-Tuning"}),", and finally ",(0,s.jsx)(n.strong,{children:"Pre-Training"}),", both the resource investment and level of achievable specificity increase."]}),"\n",(0,s.jsx)(n.h2,{id:"detailed-comparison",children:"Detailed Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsx)(n.tr,{children:(0,s.jsx)(n.th,{style:{textAlign:"center"},children:(0,s.jsx)(n.img,{alt:"Tuning Definitions",src:i(9440).A+"",width:"2316",height:"1080"})})})}),(0,s.jsx)(n.tbody,{children:(0,s.jsx)(n.tr,{children:(0,s.jsx)(n.td,{style:{textAlign:"center"},children:(0,s.jsx)(n.em,{children:"Sourced from Databricks blog"})})})})]}),"\n",(0,s.jsx)(n.p,{children:"provides a detailed table of these methods, highlighting key attributes such as:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Data Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prompt Engineering: None"}),"\n",(0,s.jsx)(n.li,{children:"RAG: External knowledge base"}),"\n",(0,s.jsx)(n.li,{children:"Fine-Tuning: Thousands of domain-specific samples"}),"\n",(0,s.jsx)(n.li,{children:"Pre-Training: Billions to trillions of tokens"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Training Time:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prompt Engineering: None"}),"\n",(0,s.jsx)(n.li,{children:"RAG: Moderate (e.g., computing embeddings)"}),"\n",(0,s.jsx)(n.li,{children:"Fine-Tuning: Moderate to long (depending on dataset size)"}),"\n",(0,s.jsx)(n.li,{children:"Pre-Training: Long (days to many weeks)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Advantages and Considerations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prompt Engineering: Fast and cost-effective, but limited control."}),"\n",(0,s.jsx)(n.li,{children:"RAG: Dynamically updated context, but longer prompts and increased inference costs."}),"\n",(0,s.jsx)(n.li,{children:"Fine-Tuning: Granular domain specialization, but requires labeled data and more compute."}),"\n",(0,s.jsx)(n.li,{children:"Pre-Training: Maximum control and tailor-fit, but extremely resource-intensive."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"choosing-the-right-approach",children:"Choosing the Right Approach"}),"\n",(0,s.jsx)(n.p,{children:"Selecting the right stage of AI tuning depends on your organization\u2019s goals, resources, and maturity level:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Immediate Results, No Additional Data:"})," Prompt Engineering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic Knowledge Integration:"})," Retrieval Augmentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Specific Domain Expertise:"})," Fine-Tuning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Full Control & Customization:"})," Pre-Training"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By understanding each stage\u2019s trade-offs, you can strategically invest in the approach that best aligns with your constraints and aspirations."})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},9440:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/4_steps_AI-c2a9ff61b6cecb7effbdadf8795f6ae3.jpg"},7580:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/model_tuning-c88d2af962a020c56676aa516f8ce8c4.png"},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}},9222:e=>{e.exports=JSON.parse('{"permalink":"/blog/ai-tuning","source":"@site/blog/Stages_of_AI_tuning.md","title":"Stages of AI Tuning","description":"The world of AI customization involves a spectrum of approaches ranging from zero-cost prompt engineering to fully-fledged pre-training of large language models (LLMs) from scratch. Each stage offers varying levels of complexity, data requirements, and organizational investment. Understanding these stages can help you choose the right approach for your specific business or research needs.","date":"2024-12-07T00:00:00.000Z","tags":[],"readingTime":2.735,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"ai-tuning","title":"Stages of AI Tuning","date":"2024-12-07T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"RAG in Trading","permalink":"/blog/rag-in-trading"}}')}}]);